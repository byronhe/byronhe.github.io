<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Search | Tech Ideas]]></title>
  <link href="https://blog.helong.info//blog/categories/search/atom.xml" rel="self"/>
  <link href="https://blog.helong.info//"/>
  <updated>2019-11-25T13:46:04+08:00</updated>
  <id>https://blog.helong.info//</id>
  <author>
    <name><![CDATA[byronhe]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[用 DAT 重实现 CppJieba 中文分词算法，降低 99% 内存消耗]]></title>
    <link href="https://blog.helong.info//blog/2019/11/25/cppjieba-darts-DAT-memory_optimize/"/>
    <updated>2019-11-25T12:34:00+00:00</updated>
    <id>https://blog.helong.info//blog/2019/11/25/cppjieba-darts-DAT-memory_optimize</id>
    <content type="html"><![CDATA[<hr />

<a name="L.................."></a>
<h2>一，问题背景</h2>

<p>中文分词应用比较广泛的开源算法，是 <a href="https://github.com/fxsjy/jieba">jieba 结巴分词</a>，结巴分词较高性能的实现是 C++ 版本的 CppJieba :
<a href="https://github.com/yanyiwu/cppjieba">https://github.com/yanyiwu/cppjieba</a></p>

<p>在实际使用 CppJieba 的过程中，我们发现 CppJieba 的内存占用比较高。</p>

<p>比如对一个 76W 词 大小 11MB 的词典 ，加载 2份 （比如为了支持平滑改动用户词典）就需要耗费 505MB内存。</p>

<p>这对一些多进程的后台服务，浪费大量内存，难以接受，因此这里希望削减内存耗费。</p>

<p>经过初步调查，确定改进方法，然后动手改造，<strong>最终把 505MB 缩减到了 4.7MB ，实现了 99% 内存降低</strong>。</p>

<p>此处也有 issue 讨论 <a href="https://github.com/yanyiwu/cppjieba/issues/3">https://github.com/yanyiwu/cppjieba/issues/3</a></p>

<p>代码稍后可能会开源出来。</p>

<!--more-->


<a name="L.................."></a>
<h2>二，实现过程</h2>

<a name="L....1................."></a>
<h3>二.1  查内存分布</h3>

<p>第一步先用 jemalloc 的 memory profiler 工具查看内存耗费在哪里，
1. 改一下 CppJieba  的  test/demo.cpp， 链接 jemalloc，编译成二进制  jieba_test<br/>
2. 然后设置环境变量
<code>
export MALLOC_CONF="prof:true,prof_prefix:mem_prof/mem_profile_je.out,lg_prof_interval:20,lg_prof_sample:20"
</code>
3.然后  mkdir mem_prof， 并运行测试程序
4.jeprof &ndash;pdf ./jieba_test mem_prof/mem_profile_je.out.xxx.heap > mem_profile.pdf</p>

<p>打开 mem_profile.pdf ，就可以看到内存分布了</p>

<a name="L....2............."></a>
<h3>二.2 优化方案</h3>

<p>显而易见，内存主要耗费在:
1. Trie.hpp 中的 Trie 树构建
2. KeywordExtractor.hpp 加载  idf 词典文件。</p>

<p>因此方案:</p>

<a name="L1..Double.Array.Trie.........cppjieba::Trie"></a>
<h4>1. Double Array Trie 代替  cppjieba::Trie</h4>

<p>引入 Double Array Trie  (简称  DAT ,<a href="https://github.com/s-yata/darts-clone">https://github.com/s-yata/darts-clone</a>) , 代替 Trie.hpp 中的简单内存 Trie，并把 darts 生成的  DAT 保存到文件中，在启动时，如果已经有和词典对应的 DAT ，直接 mmap() attach 上去，即可启动。</p>

<p>经过实测发现，75万词词典，dart-clone 生成的 DAT 文件，大小只有 24MB，而且可以 mmap 挂载，多进程共享。</p>

<a name="L2..KeywordExtractor"></a>
<h4>2. KeywordExtractor</h4>

<p>KeywordExtractor 是个不常用功能，直接改成支持传入空的 idfPath 和 stopWordPath, 此时不加载数据即可。</p>

<a name="L....3............."></a>
<h3>二.3 其他问题</h3>

<a name="L1...................................DAT......"></a>
<h4>1. 支持热更新，保证词典和DAT一致</h4>

<p>这里一个问题是，词典可能热更新，那怎么知道 DAT 文件和当前词典的内容对应？</p>

<p>我的做法是，对 默认词典文件+自定义词典文件，用文件内容算 MD5，写入 DAT 文件头部，这样打开 DAT 文件发现 MD5 不一致，就知道 DAT文件过时了，即可重建 DAT 。</p>

<p>实测发现算 MD5 还是很快的，启动时间都在 1秒 左右。</p>

<a name="L2.............."></a>
<h4>2. 代码清理</h4>

<p>另外，清理了一下代码，删掉了 Unicode.hpp 中的无用代码。
清理了 FullSegment.hpp HMMSegment.hpp MixSegment.hpp MPSegment.hpp QuerySegment.hpp 等中的重复代码。</p>

<a name="L3................."></a>
<h4>3. 不兼容改动</h4>

<ul>
<li>由于 Double Array Trie 无法支持动态插入词，删除 InsertUserWord() 方法</li>
<li>FullSegment.hpp 中 maxId 的计算有 bug，做了 fix。</li>
</ul>


<p>整体改造后，代码量比原来减少 100 多行。</p>

<p>上线后效果显著。</p>

<p>当内存降低到 2-3MB 的水平后，这意味着 75W 词这种规模的大词典，可以用在手机环境。</p>

<p>比如可以在 ios 或者 Android 上做 中文/英文的切词，
这意味着可能在客户端实现体验相当良好的搜索引擎。</p>

<p>ios 上也有可用于中文的分词器 <a href="https://developer.apple.com/documentation/corefoundation/cfstringtokenizer-rf8">CFStringTokenizer</a> ，但貌似不开源。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[GB 规模语料上的高性能新词发现算法]]></title>
    <link href="https://blog.helong.info//blog/2019/09/18/newwords_discovery/"/>
    <updated>2019-09-18T22:19:00+00:00</updated>
    <id>https://blog.helong.info//blog/2019/09/18/newwords_discovery</id>
    <content type="html"><![CDATA[<hr />

<p>分词是中文搜索的重要环节，目前分词算法已经比较成熟，分词错误的主要是由于未登录词。</p>

<p>因此发现业务领域语料库中的新词，减少未登录词，对改善搜索引擎的用户体验有重要意义。</p>

<p>新词发现的一种常用算法，是 matrix67 大神 2012 年提出的 《互联网时代的社会语言学：基于SNS的文本数据挖掘》
<a href="https://www.matrix67.com/blog/archives/5044">https://www.matrix67.com/blog/archives/5044</a></p>

<p>其主要思路，是统计语料中出现的所有 ngram 子字符串的凝固度，自由度，信息熵。</p>

<p>算法中需要统计所有 ngram 子字符串的 左熵右熵，实现该算法时，一般以子字符串为 key，用哈希表来存。</p>

<p>但随着语料库变大时，内存消耗变大，</p>

<p>比如之前的 python 版本实现，对 200MB 的语料，就需要约 30G 内存来存哈希表，</p>

<p>导致单机内存不足无法运行，而且对这样规模的语料库，算法需要跑一两天才能出结果。</p>

<p>这里我应用一些工程实现方面的技巧，
把用哈希表统计左熵右熵的计算，拆分成多个子哈希表，分批计算，并利用多核并行，大幅度优化了算法的性能。</p>

<p>最终实现了 GB 大小语料上的新词发现，并把运行时间缩减到了 30 分钟左右 。</p>

<p><a href="https://github.com/hankcs/HanLP/wiki/%E6%96%B0%E8%AF%8D%E8%AF%86%E5%88%AB">https://github.com/hankcs/HanLP/wiki/%E6%96%B0%E8%AF%8D%E8%AF%86%E5%88%AB</a></p>
]]></content>
  </entry>
  
</feed>
